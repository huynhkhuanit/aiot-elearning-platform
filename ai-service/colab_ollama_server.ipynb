{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ollama AI Server for AIoT E-Learning Platform\n",
        "\n",
        "This notebook sets up an Ollama server on Google Colab (Free T4 GPU) and exposes it via Ngrok tunnel.\n",
        "\n",
        "**Models:**\n",
        "- `deepseek-coder:1.3b` - Fast code autocomplete (FIM)\n",
        "- `codellama:13b-instruct` - Chat, code generation, review\n",
        "\n",
        "**Usage:**\n",
        "1. Run all cells in order\n",
        "2. Copy the Ngrok URL printed at the end\n",
        "3. Add it to your `.env.local` as `OLLAMA_BASE_URL`\n",
        "\n",
        "**Important:** Colab sessions expire after ~12 hours. Re-run when expired.\n",
        "\n",
        "**Chạy 24/7 miễn phí?** Xem [RUN-OLLAMA-24-7-FREE.md](./RUN-OLLAMA-24-7-FREE.md) – chạy Ollama trên PC hoặc Oracle Cloud Free Tier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check GPU Availability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU is available\n",
        "!nvidia-smi\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GPU check complete. Make sure T4 GPU is assigned.\")\n",
        "print(\"If no GPU shown, go to Runtime > Change runtime type > T4 GPU\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Ollama\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Ollama installed successfully!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Start Ollama Server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "# Start Ollama server in background\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE,\n",
        "    env={**__import__('os').environ, \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
        ")\n",
        "\n",
        "# Wait for server to start\n",
        "print(\"Starting Ollama server...\")\n",
        "for i in range(30):\n",
        "    try:\n",
        "        r = requests.get(\"http://localhost:11434/api/tags\", timeout=2)\n",
        "        if r.status_code == 200:\n",
        "            print(f\"Ollama server is running! (took {i+1}s)\")\n",
        "            break\n",
        "    except:\n",
        "        time.sleep(1)\n",
        "else:\n",
        "    print(\"WARNING: Ollama server may not have started properly.\")\n",
        "    print(\"Check stderr:\", ollama_process.stderr.read().decode()[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Pull AI Models\n",
        "\n",
        "This will download the models to the Colab VM. Takes ~2-5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull deepseek-coder:1.3b for fast autocomplete\n",
        "print(\"Pulling deepseek-coder:1.3b (autocomplete model)...\")\n",
        "print(\"This model is ~776MB, should take 1-2 minutes.\")\n",
        "print(\"=\"*60)\n",
        "!ollama pull deepseek-coder:1.3b\n",
        "print(\"\\ndeepseek-coder:1.3b ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pull codellama:13b-instruct for chat/generation\n",
        "print(\"Pulling codellama:13b-instruct (chat model)...\")\n",
        "print(\"This model is ~7.4GB, should take 3-5 minutes.\")\n",
        "print(\"=\"*60)\n",
        "!ollama pull codellama:13b-instruct\n",
        "print(\"\\ncodellama:13b-instruct ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify models are available\n",
        "print(\"\\nInstalled models:\")\n",
        "print(\"=\"*60)\n",
        "!ollama list\n",
        "\n",
        "# Quick test\n",
        "import requests\n",
        "r = requests.get(\"http://localhost:11434/api/tags\")\n",
        "models = [m['name'] for m in r.json().get('models', [])]\n",
        "print(f\"\\nAPI reports {len(models)} models: {models}\")\n",
        "\n",
        "required = ['deepseek-coder:1.3b', 'codellama:13b-instruct']\n",
        "for model in required:\n",
        "    found = any(model in m for m in models)\n",
        "    status = 'OK' if found else 'MISSING'\n",
        "    print(f\"  {model}: {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Setup Ngrok Tunnel\n",
        "\n",
        "**First time setup:**\n",
        "1. Sign up at https://ngrok.com (free)\n",
        "2. Get your auth token from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "3. Paste it below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install pyngrok\n",
        "!pip install pyngrok -q\n",
        "print(\"pyngrok installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok, conf\n",
        "import getpass\n",
        "\n",
        "# Set your Ngrok auth token\n",
        "# Option 1: Paste directly (less secure)\n",
        "# NGROK_AUTH_TOKEN = \"your-token-here\"\n",
        "\n",
        "# Option 2: Input securely (recommended)\n",
        "NGROK_AUTH_TOKEN = getpass.getpass(\"Enter your Ngrok auth token: \")\n",
        "\n",
        "# Configure and authenticate\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "print(\"Ngrok authenticated!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import requests\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Create tunnel to Ollama server\n",
        "public_url = ngrok.connect(11434, \"http\")\n",
        "OLLAMA_URL = public_url.public_url\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"NGROK TUNNEL ACTIVE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nPublic URL: {OLLAMA_URL}\")\n",
        "print(f\"\\nAdd this to your .env.local:\")\n",
        "print(f\"\\n  OLLAMA_BASE_URL={OLLAMA_URL}\")\n",
        "print(f\"  OLLAMA_COMPLETION_MODEL=deepseek-coder:1.3b\")\n",
        "print(f\"  OLLAMA_CHAT_MODEL=codellama:13b-instruct\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Verify tunnel works\n",
        "try:\n",
        "    # Ngrok free tier adds a warning page, use header to bypass\n",
        "    r = requests.get(\n",
        "        f\"{OLLAMA_URL}/api/tags\",\n",
        "        headers={\"ngrok-skip-browser-warning\": \"true\"},\n",
        "        timeout=10\n",
        "    )\n",
        "    print(f\"\\nTunnel verification: {r.status_code} OK\")\n",
        "    models = [m['name'] for m in r.json().get('models', [])]\n",
        "    print(f\"Models accessible via tunnel: {models}\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nTunnel verification failed: {e}\")\n",
        "    print(\"The tunnel may still work - try accessing the URL in your browser.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Quick Model Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "# Test deepseek-coder autocomplete (FIM)\n",
        "print(\"Testing deepseek-coder:1.3b (autocomplete)...\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "fim_prompt = \"<\\uff5cfim\\u2581begin\\uff5c>def fibonacci(n):\\n    if n <= 1:\\n        return n\\n    <\\uff5cfim\\u2581hole\\uff5c>\\n\\nprint(fibonacci(10))<\\uff5cfim\\u2581end\\uff5c>\"\n",
        "\n",
        "r = requests.post(\"http://localhost:11434/api/generate\", json={\n",
        "    \"model\": \"deepseek-coder:1.3b\",\n",
        "    \"prompt\": fim_prompt,\n",
        "    \"stream\": False,\n",
        "    \"options\": {\n",
        "        \"temperature\": 0.2,\n",
        "        \"num_predict\": 64,\n",
        "        \"stop\": [\"\\n\\n\", \"<\\uff5cfim\\u2581begin\\uff5c>\", \"<\\uff5cfim\\u2581hole\\uff5c>\", \"<\\uff5cfim\\u2581end\\uff5c>\"]\n",
        "    }\n",
        "})\n",
        "result = r.json()\n",
        "print(f\"Completion: {result.get('response', 'ERROR')}\")\n",
        "print(f\"Time: {result.get('total_duration', 0) / 1e9:.2f}s\")\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "# Test codellama chat\n",
        "print(\"Testing codellama:13b-instruct (chat)...\")\n",
        "print(\"-\"*40)\n",
        "\n",
        "r = requests.post(\"http://localhost:11434/api/chat\", json={\n",
        "    \"model\": \"codellama:13b-instruct\",\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Write a Python function to check if a string is a palindrome. Be concise.\"}\n",
        "    ],\n",
        "    \"stream\": False,\n",
        "    \"options\": {\n",
        "        \"temperature\": 0.3,\n",
        "        \"num_predict\": 256\n",
        "    }\n",
        "})\n",
        "result = r.json()\n",
        "print(f\"Response: {result.get('message', {}).get('content', 'ERROR')[:500]}\")\n",
        "print(f\"Time: {result.get('total_duration', 0) / 1e9:.2f}s\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"All tests complete! Server is ready.\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Keep Server Alive\n",
        "\n",
        "Run this cell to keep the server running. It will:\n",
        "- Ping the server every 60 seconds\n",
        "- Auto-restart Ollama if it crashes\n",
        "- Auto-reconnect Ngrok if tunnel drops\n",
        "\n",
        "**Keep this cell running!** Stopping it won't kill the server, but the health monitoring will stop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import subprocess\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from pyngrok import ngrok\n",
        "\n",
        "def check_ollama_health():\n",
        "    \"\"\"Check if Ollama server is responding.\"\"\"\n",
        "    try:\n",
        "        r = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
        "        return r.status_code == 200\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "def restart_ollama():\n",
        "    \"\"\"Restart Ollama server.\"\"\"\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Restarting Ollama...\")\n",
        "    subprocess.run([\"pkill\", \"-f\", \"ollama serve\"], capture_output=True)\n",
        "    time.sleep(2)\n",
        "    subprocess.Popen(\n",
        "        [\"ollama\", \"serve\"],\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.PIPE,\n",
        "        env={**__import__('os').environ, \"OLLAMA_HOST\": \"0.0.0.0:11434\"}\n",
        "    )\n",
        "    # Wait for server\n",
        "    for i in range(15):\n",
        "        if check_ollama_health():\n",
        "            print(f\"  Ollama restarted successfully! (took {i+1}s)\")\n",
        "            return True\n",
        "        time.sleep(1)\n",
        "    print(\"  WARNING: Ollama restart may have failed.\")\n",
        "    return False\n",
        "\n",
        "def check_ngrok_tunnel():\n",
        "    \"\"\"Check if Ngrok tunnel is still active.\"\"\"\n",
        "    tunnels = ngrok.get_tunnels()\n",
        "    return len(tunnels) > 0\n",
        "\n",
        "# Main keep-alive loop\n",
        "print(\"=\"*60)\n",
        "print(\"KEEP-ALIVE MONITOR STARTED\")\n",
        "print(f\"Ngrok URL: {OLLAMA_URL}\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nMonitoring server health every 60 seconds...\")\n",
        "print(\"Press Stop (square button) to end monitoring.\\n\")\n",
        "\n",
        "consecutive_failures = 0\n",
        "check_count = 0\n",
        "\n",
        "while True:\n",
        "    try:\n",
        "        check_count += 1\n",
        "        now = datetime.now().strftime('%H:%M:%S')\n",
        "\n",
        "        # Check Ollama health\n",
        "        ollama_ok = check_ollama_health()\n",
        "        if not ollama_ok:\n",
        "            consecutive_failures += 1\n",
        "            print(f\"[{now}] Ollama DOWN (failure #{consecutive_failures})\")\n",
        "            if consecutive_failures >= 2:\n",
        "                restart_ollama()\n",
        "                consecutive_failures = 0\n",
        "        else:\n",
        "            consecutive_failures = 0\n",
        "\n",
        "        # Check Ngrok tunnel\n",
        "        ngrok_ok = check_ngrok_tunnel()\n",
        "        if not ngrok_ok:\n",
        "            print(f\"[{now}] Ngrok tunnel lost! Reconnecting...\")\n",
        "            try:\n",
        "                public_url = ngrok.connect(11434, \"http\")\n",
        "                OLLAMA_URL = public_url.public_url\n",
        "                print(f\"  New URL: {OLLAMA_URL}\")\n",
        "                print(f\"  Update OLLAMA_BASE_URL in .env.local!\")\n",
        "            except Exception as e:\n",
        "                print(f\"  Ngrok reconnect failed: {e}\")\n",
        "\n",
        "        # Status report every 10 checks\n",
        "        if check_count % 10 == 0:\n",
        "            status = \"OK\" if ollama_ok else \"DOWN\"\n",
        "            tunnel = \"OK\" if ngrok_ok else \"DOWN\"\n",
        "            print(f\"[{now}] Status report #{check_count}: Ollama={status}, Ngrok={tunnel}, URL={OLLAMA_URL}\")\n",
        "\n",
        "        time.sleep(60)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nMonitoring stopped by user.\")\n",
        "        print(\"Server is still running. Re-run this cell to resume monitoring.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"[{datetime.now().strftime('%H:%M:%S')}] Monitor error: {e}\")\n",
        "        time.sleep(30)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
